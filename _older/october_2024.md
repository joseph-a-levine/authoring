---
layout: page
title: What am I up to now?
---

<!-- omit in toc -->
## October, 2024

<!-- omit in toc -->
## Contents
- [Updates](#updates)
- [Reading](#reading)
  - [Killing the animals, politely](#killing-the-animals-politely)
  - [Grrr transformers](#grrr-transformers)
  - [Shorts](#shorts)
- [Links](#links)
- [Previously](#previously)

<br>
  
## Updates

This was a quiet month, with lots of reading, writing, and coding, much of which I discuss below. Term starts in Oxford at the beginning of October, which doesn't herald much of a change — in the third year, we're past taking coursework and not yet at *teaching* coursework.{% sidenote "cs-id" "Except for the genius overachievers in my cohort!" %} I'll be in Oxford for all of October; as always, please say hi if you're nearby!

## Reading

### Killing the animals, politely

An acquaintance in Oxford does her PhD on rewilding. Her family runs a large rewilding project in Sussex, boasting thousands of hectares of land for wild cattle and boar and anything else that stops by. They host tours, camping, safari, and petting zoos. Gentlemen farmers had mostly given up on profitability in the 20th century, and not all estates can be rewilding attractions — but this one, it seems, is quite successful. 

Napoleon called England "a nation of shopkeepers," but talking to conservatives today, or a hundred years ago, it is not that commerce which people are nostalgic for. England, in its own memory, is a nation of farmers, who shape and rule their land and its bounty. Shooting, in the Victorian style, with a gameskeeper and beaters and loaders, was something wholly and synonmously associated with this England. 

*The Shooting Party* by Isabel Colegate is set in 1913 and written in the 1970s; it mocks, memorializes, and mourns this bit of pastorality. Sir Randolph Nettleby, Baronet, hosts the best shoots in England at his estate five miles west of Oxford, and he is quite proud that they were a favorite of King Edward VII. 

Colegate touchingly substitutes the several hundred birds killed that day with a martyr, a duck named Elfrida Beetle{% sidenote "elfred-id" "'She used to be called Alfred and then when she turned into a female she was going to be called Alfreda but Nanny said it wasn't Alfreda but Elfreda and then she'd just swallowed a beetle and elle is French for she.'" %} adopted by the Baronet's grandson. The duck escapes on the day of the duck hunt, among the ducks to be hunted, forcing a moment of vertigo among the shooters. Even the hunters in the line, with three guns to hand, pledge they "shall not, if I can possibly help it, shoot young Osbert's duck."

The shooters do not wish to be rude by killing the child's duck. That is not, however, the focus of their politeness. The driving conflict of the story is between Lord Gilbert Hartlip ("one of the best shots in England, if not the best of all, and it was a pleasure to see him in action") and Lionel Stephens — a more fit, modern sportsman. Throughout the party and the affairs and the secrets — predictable to any fan of Wodehouse or *Downton Abbey* — the greatest tension is in these men's unsporting competition. 

The climax of the book, without any spoiler, is Sir Randolph's admonishment: "You were not shooting like a gentleman, Gilbert." Because, “A greedy shot in line is tiresome and an ideal day requires participants to be considerate about which birds they attempt to shoot.”

The second quote is not from Colegate's novel, but a more recent and puzzling book. Matthew Scully, a speechwriter for Bush the Younger and columnist at the *National Review*, thus introduces George H.W. Bush in his book *Dominion.*{% sidenote "dombo-id" "This is not the Tom Holland book, as people think when I bring it up, and unrelated to the movie of the same title and topic." %}  Bush was giving a speech to the Safari Club convention, the largest gathering of hunting enthusiasts in America, and Scully was present, riding his formidable conservative Christian credentials without anyone knowing he was writing a book condemning, widely and forcefully, our treatment of animals.{% sidenote "bushfor-id" "The former president, never a big game hunter, 'seems a man not quite comfortable with his surroundings yet determined not to offend.'" %}

Scully's case for vegetarianism is, like him, conservative, compassionate, and religious. The book is full of biblical verse and commentary, used waggingly like a formerly-homeschooled, now-atheist kid talking back to their still-evangelical parents. Scully is, tho, fully sincere. The title is from Genesis 1:28: 

> And God blessed them, and God said unto them, Be fruitful, and multiply, and replenish the earth, and subdue it: and have **dominion** over the fish of the sea, and over the fowl of the air, and over every living thing that moveth upon the earth.

Humans, he argues, remember the "subdue" part of our dominion, but not the duties of mercy and charity which come with it. Scully interleaves his investigations with moral musing.{% sidenote "inves-id" "It's too generous to label Scully an undercover reporter. He never lies, really — everyone just assumes that as a Republican, he supports big game hunting, whaling, and America's Farms. He is puzzled by this, but willing to take advantage." %} He draws you in with condemnations of hunting and whaling, and then wallops you over the head with factory farming. This is not what he intended: 

> I debated long and hard with myself about whether to include farm animals in this book. Holding off on that topic for another day would certainly spare me a lot of trouble, and maybe even gain me a wider readership (“Oh, good, at least he’s not one of those people”).

Instead, he spends more pages and pathos on the hunters, for their cruelty, certainty, but also their hypocracy and discourtesy. 

This is not to fault the chapter on factory farming: Scully does not shy away from the suffering of the pigs he visits; like Power, he cannot avoid a Holocaust reference. But his description of horrors is purposefully clinical. He does not get angry at the managers and staff of the farms. Rather, disappointed, he sees them as clueless, out of touch, and blind. 

We are all only human. Scully seems to resist throwing numbers at us, of the scale of our cruelties. He has little truck for Peter Singer, whom he (ironically) respects for bringing animal welfare to the mainstream, but abhors for what brought him there. We are all only human, and a moment of disgust will be worth much more than the number 4,000,000.{% sidenote "pigday-id" "The number of pigs killed, every day." %} Scully, echoing Sir Randolph, finds humanity's treatment of animal ungentlemanly, or at least un-manly, and therefore ungodly.


### Grrr transformers

Two weeks ago, I got a sudden urge to understand how [transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) worked. This was directly caused by learning how [the other kind of transformer](https://en.wikipedia.org/wiki/Transformer) works — it was intolerable to me that I understand the piece of technology which revolutionized the 1890s and not the piece shaping the world today. If that sounds like a dumb reason to spend 50 hours reading machine learning papers, well, it is.{% sidenote "karpto-id" "I was learning about electricial transformers for a completely unrelated project, on Turkish boats which generate and provide energy to small west African countries." %}

It's surprising that my curiosity about AI had never showed up. I've used ChatGPT since December 2022.{% sidenote "furthtu-id" "Going even further back, I first thought deeply about AI (as deeply as 19 year old me could) in 2015, evidenced by an embarassing email I sent to a girl I had a crush on." %} I work in a building with the Center for the Governance of AI. Nick Bostrom is there sometimes. I'm one or two phone calls away from the authors of papers (and models) any of you have heard of. My girlfriend does really cool AI stuff. Yet I never had any  motivation to figure out what was going on.

Of course, the problem with wanting to learn how transformers work is that *no one* understands how they work: there are a bunch of very smart people at [Google](https://deepmind.google/research/publications/22295/) and [Anthropic](https://www.anthropic.com/research/mapping-mind-language-model) etc. who want to know this much more than I do.{% sidenote "mechi-id" "I'm being facetious: mechanistic interpretability seeks to understand models made up of transformers, not the transformers themselves. But that's what I want, too." %} But the building blocks, the math bits, are out there for the understanding. 

And relatively achievable: I now understand transformers to the point where I've manually coded a few small models, including GPT-2, and I have no great insight. (My curriculum is in this sidenote.{% sidenote "curric-id" "Read [this](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=pndoEIqJ6GPvC1yENQkEfZYR). Read and implemented [this](https://nlp.seas.harvard.edu/annotated-transformer/). Read and *loved* [this](https://transformer-circuits.pub/2021/framework/index.html). Coded [this](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo_Template.ipynb). Read [this](https://jalammar.github.io/illustrated-transformer/) and [this](https://blog.nelhage.com/post/transformers-for-software-engineers/). Did the exercises in [this](https://arena-ch1-transformers.streamlit.app/[1.5]_Grokking_and_Modular_Arithmetic)." %}) My curiosity about how these things work — about how these AIs work — has just been moved up a level. It's like — yesterday I was confused about how motorcycles go so fast, and today I know exactly how pistons work, but there's no one on the *planet* who can explain what crankshafts and camshafts do, so I'm left with this really cool and intricate bit of knowledge which doesn't sate me.

There's a longer essay in here, and I had outlined it; I even knew the title: *And all to construct some versifier!* Stanislaw Lem and his translator Michael Kandel conspired to write the greatest short story about LLM-powered chatbots back in the 1960s, titled "The First Sally (A), or Trurl's Electronic Bard" in *The Cyberiad.*{% sidenote "cbq-id" "This is one of my favorite books; in a review from before I had ever heard of LLMs, I [quote](https://jablevine.com/older/july_2021#books) another story from the collection." %} Trurl, a mechanical constructor, set out to build a machine which can write poetry. He starts by training it on the history of civilization — he "could move at this rate because his machine was able, in one five-billionth of a second, to simulate one hundred septillion events at forty octillion different locations simultaneously." And, as I planned to cleverly title my essay, "all to construct some versifier!"

The first time Trurl turns on the machine, "it immediately proceeded to deliver a lecture on the grinding of crystallographical surfaces as an introduction to the study of sub-molecular magnetic anomalies." This is a puzzle familiar to anyone who played with early LLMs: it is easier for them to perform technical tasks than to produce doggerel. Yet the machine trains more, and is fixed, and versifies. And then, if there's any part of the book that's well-known, it's this: Trurl's competitor prompts it for

> a love poem, lyrical, pastoral, and expressed in the language of pure mathematics. Tensor algebra mainly, with a little topology and higher calculus, if need be. But with feeling, you understand, and in the cybernetic spirit.

Kandel translates the poem into eight rhyming quatrains, and I [recommend it entirely](https://www.cse.wustl.edu/~jbuhler/cyberiad.html): 

> Come, let us hasten to a higher plane,
> 
> Where dyads tread the fairy fields of Venn,
> 
> Their indices bedecked from one to *n*,
> 
> Commingled in an endless Markov chain!

I'm getting somewhere with this. As relevant as Lem's *Cyberiad* is to the current confusion and rhetoric around LLMs, it's not surprising someone else wrote about it first. It's less surprising that someone was [Gwern](https://gwern.net/gpt-3#stanislaw-lems-cyberiad).

As my essay was coming together, GPT o1 was released and people put it through its paces. One of Gwern's standard tests for new models is a prompt from the same Lem story: 

> Have it compose a poem---a poem about a haircut! But lofty, noble, tragic, timeless, full of love, treachery, retribution, quiet heroism in the face of certain doom! Six lines, cleverly rhymed, and every word beginning with the letter 's'!

This is a hard test, for reasons I now more-or-less understand.{% sidenote "obv-id" "LLMs work in one direction, and writing a poem like this requires planning from the end, almost — where does the treachery come in, how do I finish the rhyme, etc. Further, LLMs don't see *letters*, they see sub-words. Starting each word with the letter 's' implies knowing what an 's' is, which is hard for LLMs." %} I couldn't do it at all, and good poets would struggle. When Gwern gave the new OpenAI model this prompt, it got *very* close on its third try: 

> Silently, steel scissors slice, 
> 
> Silken strands shed, shadows splice.
> 
> Steadfast soldier, scarred, serene,
> 
> Sacred sacrifice seals solemn scene.
> 
> Swiftly sweeps *t*he sable storm,
> 
> Solitary soul seeks shelter's swarm.

Only one word fails to start with 's', and the poem does fairly well on the other constraints. The output when prompted to fix it, however, is surprising for different reasons:

> Seduced, shaggy Samson snored.
> 
> She scissored short. Sorely shorn,
> 
> Soon shackled slave, Samson sighed,
> 
> Silently scheming
> 
> Sightlessly seeking
> 
> Some savage, spectacular suicide.

As Gwern points out, *this is the poem from* The Cyberiad.{% sidenote "gopo-id" "It's also, in my opinion, quite a fun poem. I memorized it a few years ago, on a re-read of Kandel's translation, as a piece of doggerel to keep in my pocket. I'm glad it was Gwern who saw GPT o1 output it, because I'm not sure how I would have reacted." %} This is... weird. We're mostly past "AI is plagiarism" discourse at this point, but there was a time when the mainstream worry was LLMs stealing human writing word-for-word. These models are probabilistic, so their dominant mode isn't just outputting exact text from within their training data. Further, this is a good way to get sued, so we'd think that OpenAI would try very hard to discourage direct copying while training. Gwern's explanation is that, due to the tight constraints, 

> a highly-discouraged, unlikely solution suddenly becomes accessible. GPT-4 o1-preview jumped out of the local optimum to completely change the answer to something that is technically more correct (it does satisfy all the constraints & is a good poem) but undesirable (because just memorized plagiarism, which I don't want, and further, ChatGPT is tuned to avoid that, although in this case the copyrighted text is brief enough that it doesn't outright violate its guidelines and you don't need to jailbreak it).

Lem and Kandel's story is excellent and weird and prescient, and (indirectly) revealing insights into technologies invented sixty years after writing. I'm not going to understand LLMs anytime soon, but I'm glad poetry is a useful place to mine insights. Alfréd Rényi, a Hungarian mathematician well known for work on random graphs, would be happy: 

> Since I started to deal with information theory I have often meditated upon the conciseness of poems; how can a single line of verse contain far more ‘information’ than a highly concise telegram of the same length. The surprising richness of meaning of literary works seems to be in contradiction with the laws of information theory. The key to this paradox is, I think, the notion of ‘resonance’. The writer does not merely give us information, but also plays on the strings of the language with such virtuosity, that our mind, and even the subconscious self resonate. A poet can recall chains of ideas, emotions and memories with a wellturned word. In this sense, writing is magic.


### Shorts

{% newthought "Márquez is special." %} I've never read either of the masterpieces, *Cholera* or *Solitude*, but extensively through his journalism and non-fiction. *Chronicle of a Death Foretold* is a mystery novel in the style of journalism; the narrarator is explicitly a journalist. There is no underlying mystery; in fact, it turns the genre on its head. The "whodunit" elements are all revealed on the first page (who, when, where, why), but the story gets murkier instead of clearer as the book progresses. It's also startlingly funny. 

The English title is now inscribed, even new translations retain it. "Foretold" is translated from "anunciada" which doesn't have the same implications of destiny and inevitability. These are certainly themes in the story, especially with early and late discussions of portents seen by the murdered's mother. But it is a bit of over-translation. 

{% newthought "I read Milton's" %} *Samson Agonistes* twice through this month, and connected much more to it than I did *Paradise Lost*, or any other blank verse. 

{% newthought "Here's a great review" %} from the CIA of a [book](https://www.cia.gov/resources/csi/static/review-spies-in-congo.pdf) I probabaly won't read, on American efforts to secure uranium ore from the Congo during WWII. History is really big, and I don't have time for all of it. 

{% newthought "I enjoyed Jia Tolentino's" %} book of essays trick mirror, directly after the Schwarz compilation. Tolentino thrived in very early internet culture, writing hundreds of words a day on various sites from the age of eight. Her massive reserves of social energy are evident on every page; there's a very critical [review](https://www.lrb.co.uk/the-paper/v42/n02/lauren-oyler/ha-ha!-ha-ha) which didn't resonate with my own lack of connection. I came away underwhelmed. Tolentino mentions *Samson Agonistes* twice, both of which felt out of place. 

{% newthought "I was frustrated by" %} *The Midnight Library*, which had come recommended by *three* friends. It was a modern morality tale, with an over constructed and under explained magic/unreality system. A superheroine commits suicide ("My therapist says it's not clinical depression, it's situational. I just have a lot of situations.") because she had a bad day (cat died, got fired, neglected her student, got told to piss off by her best friend and neighbor). BUT! She's special and gets to decide to turn back any of her poor decisions. She can slide between lives. There is either a serious misunderstanding of talent (I think it's this one) or this book takes the many-worlds interpretation seriously. There are easily accessible worlds (which she discusses before dying!) where she is a Michael Phelps-level retired swimmer, where she is a Hayley Williams-level rock star, where she's a professor of philosophy at Cambridge. That's just not how skill is distributed. BUT, besides the nitpick, I didn't really mind the book too much. I ended up skimming the final 100 pages quickly, as it became clear that she would get everything she wanted. There's no lesson in here, despite being constructed around one. Or if there is a lesson, it's "extrapolated volition" or whatever. 


## Links

{% newthought "``Amtrak's Penn Station" %} operations team seems eternally surprised as trains arrive." On [games in everyday life](https://warofyesterday.blogspot.com/2010/05/games.html).

{% newthought "Last month, I" %} [talked](https://jablevine.com/older/september_2024#links) about why Rome didn't have an industrial revolution. [Here](https://glineq.blogspot.com/2016/05/economic-reflections-on-fall-of.html) are more considerations, this time from the inimitable Branko Milanovic. He considers Byzantium: 

> I do not know why market economy with wage labor failed to develop there and can only speculate that it might have been because of a militarized bureaucracy, land magnates (and thus high inequality), obsession with Christian theology which sucked the best minds into sterile disputes (it would be nice to have an anti-Christian like Gibbon tell us why the Eastern Empire could not become a capitalistic power!), its frequent wars with Arabs, Persians, Russians, Normans, Bulgars, Pechinegs, Avars, Ottomans… Any other candidates?

{% newthought "I came across the phrase" %} "balls to the wall" in one of Admiral Cloudberg's recent articles, and the [etymology](https://en.wiktionary.org/wiki/balls_to_the_wall
) was not what I was expecting. Also, unrelated to to the etymology [balls out](https://en.wiktionary.org/wiki/balls-out). 

{% newthought "Tulsa, and especially its Jews" %}, is doing [crazy things](https://trevorklee.substack.com/p/something-interesting-is-happening) to attract migrants. No one worry; I have no desire to move to Oklahoma. But the fundamental message of “Move to Tulsa and you are guaranteed a job, a nice house, a fun social life, and assistance fulfilling whichever dreams you have” is attractive.



## Previously

*[September, 2024](https://jablevine.com/older/september_2024)*

*[August, 2024](https://jablevine.com/older/August_2024)*

*[July, 2024](https://jablevine.com/older/july_2024)*

*[June, 2024](https://jablevine.com/older/june_2024)*

*[May, 2024](https://jablevine.com/older/may_2024)*

*[April, 2024](https://jablevine.com/older/april_2024)*

*[March, 2024](https://jablevine.com/older/march_2024)*

*[February, 2024](https://jablevine.com/older/february_2024)*

*[January, 2024](https://jablevine.com/older/january_2024)*

*[December, 2023](https://jablevine.com/older/December_2023)*

*[November, 2023](https://jablevine.com/older/November_2023)*

*[October, 2023](https://jablevine.com/older/October_2023)*

*[September, 2023](https://jablevine.com/older/September_2023)*

*[August, 2023](https://jablevine.com/older/August_2023)*

*[July, 2023](https://jablevine.com/older/July_2023)*

*[June, 2023](https://jablevine.com/older/June_2023)*

*[May, 2023](https://jablevine.com/older/May_2023)*

*[April, 2023](https://jablevine.com/older/April_2023)*

*[March, 2023](https://jablevine.com/older/march_2023)*

*[February, 2023](https://jablevine.com/older/february_2023)*

*[December, 2022](https://jablevine.com/older/december_2022)*

*[November, 2022](https://jablevine.com/older/november_2022)*

*[October, 2022](https://jablevine.com/older/october_2022)*

*[September, 2022](https://jablevine.com/older/september_2022)*

*[August, 2022](https://jablevine.com/older/august_2022)*

...

*[November, 2021](https://jablevine.com/older/november_2021)*

*[October, 2021](https://jablevine.com/older/october_2021)*

*[September, 2021](https://jablevine.com/older/september_2021)*

*[July, 2021](https://jablevine.com/older/july_2021)*

*[June, 2021](https://jablevine.com/older/june_2021)*

*[May, 2021](https://jablevine.com/older/may_2021)*